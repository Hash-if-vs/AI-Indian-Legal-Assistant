{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# APPLICATION\n\n- This notebook helps you run the application on a jupyter notebook.\n- The web app interface is developed using streamlit framework\n- The pipelines we have seen in the previous notebooks are integrated here to giveout a clean application\n- Please note that the preprocessing step is not integrated here, therefore makesure that you have your vector database loaded into the notebook before running.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-06T06:14:19.844921Z","iopub.execute_input":"2024-06-06T06:14:19.845190Z"}}},{"cell_type":"code","source":"# Run, ignore all the warnings\n!npm install -q localtunnel\n!pip install -q -U bitsandbytes\n!pip install -q sentence_transformers\n!pip install -q accelerate==0.21.0 transformers==4.31.0 tokenizers==0.13.3\n!pip install -q einops==0.6.1\n!pip install -q xformers==0.0.22.post7\n!pip install -q langchain==0.1.4\n!pip install -q chromadb FlagEmbedding\n!pip install -q streamlit_chat streamlit\n!pip install -q pip install streamlit-option-menu pypdf2 together pdfplumber\n","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:03:21.539760Z","iopub.execute_input":"2024-06-13T10:03:21.540100Z","iopub.status.idle":"2024-06-13T10:10:20.673901Z","shell.execute_reply.started":"2024-06-13T10:03:21.540072Z","shell.execute_reply":"2024-06-13T10:10:20.672856Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[K\u001b[?25hm##################\u001b[0m] | reify:yargs: \u001b[32;40mhttp\u001b[0m \u001b[35mfetch\u001b[0m GET 200 https://registry.npmjs.o\u001b[0m\u001b[Kpmjs.o\u001b[0m\u001b[K\nadded 22 packages in 2s\n\n3 packages are looking for funding\n  run `npm fund` for details\n\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m \n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m New \u001b[33mminor\u001b[39m version of npm available! \u001b[31m10.1.0\u001b[39m -> \u001b[32m10.8.1\u001b[39m\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m Changelog: \u001b[36mhttps://github.com/npm/cli/releases/tag/v10.8.1\u001b[39m\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m Run \u001b[32mnpm install -g npm@10.8.1\u001b[39m to update!\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m \n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.31.0 which is incompatible.\nsentence-transformers 3.0.1 requires transformers<5.0.0,>=4.34.0, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.0.1 requires transformers<5.0.0,>=4.34.0, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.9.3 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\nsentence-transformers 3.0.1 requires transformers<5.0.0,>=4.34.0, but you have transformers 4.31.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 30.1.0 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.9.3 requires keras-core, which is not installed.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.6 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"\n\"\"\" If you are on kaggle notebooks \n        1. Upload your vectordatabase in zip format to google dive.\n        2. Copy the file id form the file link.\n        3. Paste the id below. \n        4. Download and unzip the vectordb by running this cell\n        5. Ignore this cell if you are not on kaggle\"\"\"\n\n!conda install -q -y gdown\n!gdown -q --id 1pSOOesrWzNRb3hrb-erFcEox6Tlz02Yh\n!unzip -q vectordb2.zip\n","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:24.581122Z","iopub.execute_input":"2024-06-13T10:15:24.581519Z","iopub.status.idle":"2024-06-13T10:15:46.541142Z","shell.execute_reply.started":"2024-06-13T10:15:24.581466Z","shell.execute_reply":"2024-06-13T10:15:46.539945Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## INITIALIZER\n- This will load all the models and pipelines, make sure to give proper paths and locations of your vectordatabase.\n- Try experimenting with different models.\n- Make sure that you use the same embedding model used for preprocessing.\n- Together ai API is used here to use a mistral model with a higher context length for summarisation of large casefiles , make sure that you plugin your api key to the arguement together_api_key inside the code\n- Get the api key here: [together](https://api.together.ai/)\n","metadata":{}},{"cell_type":"code","source":"\n%%writefile mainlib.py\n\n\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_community.llms import Together\nfrom langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.llms import HuggingFacePipeline\nfrom transformers import StoppingCriteria, StoppingCriteriaList\nimport torch\nimport langchain\nfrom torch import cuda, bfloat16\nimport transformers\nimport streamlit as st\nfrom streamlit_chat import message\nimport re\nimport os\nchatmodel='Hashif/Indian-legal-Llama-2-7b-v2'\nmodel_name = \"BAAI/bge-base-en\"\nencode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\nmodel_norm = HuggingFaceBgeEmbeddings(\n    model_name=model_name,\n    model_kwargs={'device': 'cuda'},#change device to 'cpu' if gpu is not available\n    encode_kwargs=encode_kwargs\n  )\ndef load_pipeline(model_name):\n  model_id = model_name\n  device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n  # set quantization configuration to load large model with less GPU memory\n  # this requires the `bitsandbytes` library\n  bnb_config = transformers.BitsAndBytesConfig(\n     load_in_4bit=True,\n      bnb_4bit_quant_type='nf4',\n      bnb_4bit_use_double_quant=True,\n      bnb_4bit_compute_dtype=bfloat16\n    )\n\n  # begin initializing HF items, you need an access token\n  model_config = transformers.AutoConfig.from_pretrained(\n      model_id,\n      #use_auth_token=hf_auth\n  )\n\n  model = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n    device_map='auto',\n    #use_auth_token=hf_auth\n  )\n  model.eval()\n  tokenizer = transformers.AutoTokenizer.from_pretrained(\n    model_id,\n    #use_auth_token=hf_auth\n    )\n  stop_list = ['\\nHuman:', '\\n```\\n','\\n\\n']\n\n  stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n  stop_token_ids\n  stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n  class StopOnTokens(StoppingCriteria):\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        for stop_ids in stop_token_ids:\n            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n                return True\n        return False\n\n  stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n  generate_text = transformers.pipeline(\n    model=model,\n    tokenizer=tokenizer,\n    return_full_text=True,  # langchain expects the full text\n    task='text-generation',\n    # we pass model parameters here too\n    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n    max_new_tokens=2048,\n    repetition_penalty=1.1  # without this output begins repeating\n      )\n  return generate_text\n\ndef make_chain(llm):\n\n  persist_directory = '/kaggle/working/vectordb2'#edit this line to \n  vectordbs = Chroma(\n  persist_directory=persist_directory,\n  embedding_function=model_norm\n    )\n  memory = ConversationBufferMemory(\n  memory_key=\"chat_history\",\n    return_messages=True)\n  retriever = vectordbs.as_retriever(search_kwargs={\"k\": 3})\n  qa = ConversationalRetrievalChain.from_llm(\n  llm,\n  retriever=retriever\n  #memory=memory\n  )\n  return qa\n\n\n\n\n\n   \n\n  \n\n\ndef make_sumllm():\n  llm = Together(\n   model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n   temperature=0.7,\n      max_tokens=1500,\n      top_k=1,\n      together_api_key=\"insert your api key here\"\n        )\n  return llm\n  \n\n    \nsumllm=make_sumllm()\npipeline=load_pipeline(chatmodel)\nllm = HuggingFacePipeline(pipeline=pipeline)\nqa=make_chain(llm)\n\n\n   \n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-13T11:29:29.196080Z","iopub.execute_input":"2024-06-13T11:29:29.197009Z","iopub.status.idle":"2024-06-13T11:29:29.207201Z","shell.execute_reply.started":"2024-06-13T11:29:29.196975Z","shell.execute_reply":"2024-06-13T11:29:29.205776Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Overwriting mainlib.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## APP","metadata":{}},{"cell_type":"code","source":"%%writefile app.py\n\n\nimport streamlit as st\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom streamlit_option_menu import option_menu\nfrom mainlib import sumllm,qa,pipeline,llm\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport json\nfrom PyPDF2 import PdfReader\nimport PyPDF2.errors\nimport base64\nfrom streamlit_chat import message\nfrom langchain.chains import ConversationalRetrievalChain\nimport requests\nimport pdfplumber\nfrom typing import Callable, List, Tuple, Dict\nfrom langchain.docstore.document import Document\nimport PyPDF2\nimport re\ndef conversational_chat(chain,query):\n  if 'petition' in query:\n    prompt=f'''<INST>#Instruction: You are a legal advisor, give services to the clients like drafting petitions, clearing doubts and providing legal assistance according to their queries\n                #client:{query}\n                #Answer: </INST>'''\n    res=pipeline(prompt)\n    trim=res[0][\"generated_text\"].split('</INST>')\n    output=trim[1]\n    st.session_state['history'].append((query,output))\n  else:\n    prompt=f'''#Instruction: You are a legal advisor, give services to the clients like drafting petitions, clearing doubts and providing legal assistance according to their queries\n                        #client:{query}\n                        #Answer: '''\n    \n    result = chain({\"question\": prompt,\"chat_history\":st.session_state['history']})\n    st.session_state['history'].append((query,result[\"answer\"]))\n    output=result['answer']\n  return output\ndef conversational_chat2(chain,query):\n  result = chain({\"question\": query,\"chat_history\":st.session_state['history2']})\n  st.session_state['history2'].append((query,result[\"answer\"]))\n  output=result['answer']\n  return output\n\ndef chat2(chain):\n    with st.empty().container():\n         \n        if 'history2' not in st.session_state:\n            st.session_state['history2']=[]\n        if 'generated2' not in st.session_state:\n            st.session_state['generated2']=[\"Hello how can I assist you?\"]\n        if 'past2' not in st.session_state:\n            st.session_state['past2']=[\"Hey!\"]\n\n        response_container=st.container()\n        container=st.container()\n        with container :\n            with st.form(key=\"my_form\",clear_on_submit=True):\n                user_input= st.text_input('query:',placeholder=\"Type in here...\",key='input')\n                submit_button= st.form_submit_button(label='chat')\n            if submit_button and user_input:\n                output= conversational_chat2(chain,user_input)\n\n                st.session_state['past2'].append(user_input)\n                st.session_state['generated2'].append(output)\n        if st.session_state['generated2']:\n            with response_container:\n                for i in range(len(st.session_state['generated2'])):\n                    message(st.session_state[\"past2\"][i], is_user=True,key=str(i)+'_user',avatar_style=\"big-smile\")\n                    message(st.session_state['generated2'][i],key=str(i),\n                            avatar_style='thumbs')\n  \ndef chat(chain):\n    with st.empty().container():\n         \n        if 'history' not in st.session_state:\n            st.session_state['history']=[]\n        if 'generated' not in st.session_state:\n            st.session_state['generated']=[\"Hello how can I assist you?\"]\n        if 'past' not in st.session_state:\n            st.session_state['past']=[\"Hey!\"]\n        st.title(\"Legal assistant\")\n        response_container=st.container()\n        container=st.container()\n        with container :\n            with st.form(key=\"my_form\",clear_on_submit=True):\n                user_input= st.text_input('query:',placeholder=\"Type in here...\",key='input')\n                submit_button= st.form_submit_button(label='chat')\n            if submit_button and user_input:\n                output= conversational_chat(chain,user_input)\n\n                st.session_state['past'].append(user_input)\n                st.session_state['generated'].append(output)\n        if st.session_state['generated']:\n            with response_container:\n                for i in range(len(st.session_state['generated'])):\n                    message(st.session_state[\"past\"][i], is_user=True,key=str(i)+'_user',avatar_style=\"big-smile\")\n                    message(st.session_state['generated'][i],key=str(i),\n                            avatar_style='thumbs')\n\ndef home():\n    background_image = \"\"\"\n    <style>\n    [data-testid=\"stAppViewContainer\"] > .main {\n        background-image: url(\"https://lh3.googleusercontent.com/pw/AP1GczNg67lujMgcHYeTVH3oUhDvFJ9s-kl7RYhxQSIHfbAsQmkh8hPaUCTx3-5O7IVZjy1RICX1NyH1qzg5q4sHxf_1fhtkvG0dAHTb0RKBrxfXfrmVfrnnpdBjVJ-skB4QBW1oCEV4THn0BDhbMzbxrdvIrg5oin8LQ-8JUDKX-2oowUxfzSZnYrGTeXF8EBNQ66k1yuRE36_rScGZrBli_wfkg2Pg0jO0VxTwk4PkmJSAKRpRGTEVXHECgGdLDaHA6JKb34mROZuvrDUZ3hZ-c87P1-_6LJo2bedbb6xhPthUAapdVHcdWv9zk96mgOaBpaXRNLNYxnJV0OqMaVKtmIctQu_Iyze_XDyjmzFGlMk-iIPkhcjQn9MKBarGxeACYIZP6tySSytJuKP5AXF10ZNs99nx97RDPn8ieaK3GiycD8IAPLD-n8SucqI7Gp1xAW6PI4u2ANugpyJcgSsOFGbxgNfwToYyon4JMk2uPNfE_xfZryon13aCaA2FVXUxwKk7cjvb8ggmaYleR2LC0ga4GsKSYE8J6dFtuY-ukAQlSlj7vkDhty3y2uqQK6kcbNl3hISrkcS_ejk2uLH5j-ft2XH6qiFbfwtKmVCuiqwxKoZ1-bOetf0IDVCPJPLrafDn4Y2j9p5CM3vpjqjRyYiXN3zjt4gU9fCy9Goet3dJHxe-Lxgj0OoXpQ_YBSSswMd2NXFvobMgeM2JohMx12-_FSMD00KViXjOF6GuQguZLGXh8oaqw2PgXaq59or_q144bLpjkts5oKgJTIY9KaC15S4lQ6phtM9t_V2v6SdO-R-f0cn99Mhdbokr21p4s5cGvsms07e7FiEYlQ3fTXG_jZIRVU7jsrZSwhF7eZN--Ae2rrjMUSfE38oqiBJKp1mXsZFkNMQ-xZNGvvNfBGU=w1000-h625-s-no-gm?authuser=0\");\n        background-size: 90vw 100vh;  # This sets the size to cover 100% of the viewport width and height\n        background-position: center;  \n        background-repeat: no-repeat;\n    }\n    </style>\n    \"\"\"\n\n    st.markdown(background_image, unsafe_allow_html=True)\n\n\n   \n    # Your app content goes here\n    st.title(\"Welcome to Legal Assistant\")\n    st.write(\"Your virtual legal companion for quick assistance.\")\n    st.header(\"Why Legal Knowledge is Necessary:\")\n    st.write(\"Legal knowledge is essential for individuals and businesses alike to navigate the complexities of the legal system. Here are a few reasons why it's crucial:\")\n\n    st.markdown(\"- **Compliance:** Understanding legal requirements helps ensure compliance with laws and regulations, avoiding penalties and legal liabilities.\")\n    st.markdown(\"- **Risk Mitigation:** Knowledge of legal principles enables individuals and businesses to identify and mitigate legal risks, protecting their interests.\")\n    st.markdown(\"- **Protection of Rights:** Legal knowledge empowers individuals to assert their rights and interests effectively, whether in contracts, disputes, or other legal matters.\")\n    st.markdown(\"- **Business Operations:** For entrepreneurs and businesses, legal knowledge is vital for structuring operations, managing contracts, and safeguarding intellectual property.\")\n\n    # How Legal Knowledge is Empowering\n    st.header(\"How Legal Knowledge is Empowering:\")\n    st.write(\"Beyond mere compliance, legal knowledge empowers individuals and businesses in various ways:\")\n\n    st.markdown(\"- **Confidence:** With a solid understanding of the law, individuals and businesses can approach legal matters with confidence, knowing their rights and obligations.\")\n    st.markdown(\"- **Decision-Making:** Legal knowledge enables informed decision-making, guiding choices in business strategies, transactions, and risk management.\")\n    st.markdown(\"- **Advocacy:** Armed with legal knowledge, individuals can advocate for themselves effectively in legal proceedings, negotiations, and interactions with authorities.\")\n    st.markdown(\"- **Innovation:** Understanding legal frameworks fosters innovation by providing clarity on intellectual property rights, licensing, and regulatory landscapes.\")\n\n    # Features section\n    st.header(\"What we Offer\")\n    st.markdown(\"- **Legal Assistance** Get answers to your legal quries. Get legal guidance from the legal assistant chatbot. Get empowered by legal Knowledge.\")\n    st.markdown(\"- **Judgement Summarisation:** Get crisp summaries for court judgements.\")\n    st.markdown(\"- **Legal Document Drafting:** Generate legal documents with ease.\")\n    st.markdown(\"- **Legal Research:** Upload your legal documents and chat with it, speed up your legal research.\")\n    st.markdown(\"---\")\n \n    \ndef read_credentials(filename):\n    with open(filename, 'r') as file:\n        credentials = json.load(file)\n    return credentials\ndef write_credentials(filename, new_username, new_password):\n    credentials = read_credentials(filename)\n    credentials[\"users\"].append({\"username\": new_username, \"password\": new_password})\n    with open(filename, 'w') as file:\n        json.dump(credentials, file, indent=4)\ndef authenticate(username, password):\n    # Check if the username exists and if the password matches\n    user_credentials = read_credentials(\"/kaggle/working/project/credentials.json\")\n    for i in range(len(user_credentials[\"users\"])):\n      if username == user_credentials[\"users\"][i][\"username\"] and user_credentials[\"users\"][i][\"password\"] == str(password):\n          return True\n    return False\ndef is_logged_in():\n    global status  # Use global status variable\n    return status\ndef login():\n    background_image = \"\"\"\n    <style>\n    [data-testid=\"stAppViewContainer\"] > .main {\n        background-image: url(\"https://lh3.googleusercontent.com/pw/AP1GczNg67lujMgcHYeTVH3oUhDvFJ9s-kl7RYhxQSIHfbAsQmkh8hPaUCTx3-5O7IVZjy1RICX1NyH1qzg5q4sHxf_1fhtkvG0dAHTb0RKBrxfXfrmVfrnnpdBjVJ-skB4QBW1oCEV4THn0BDhbMzbxrdvIrg5oin8LQ-8JUDKX-2oowUxfzSZnYrGTeXF8EBNQ66k1yuRE36_rScGZrBli_wfkg2Pg0jO0VxTwk4PkmJSAKRpRGTEVXHECgGdLDaHA6JKb34mROZuvrDUZ3hZ-c87P1-_6LJo2bedbb6xhPthUAapdVHcdWv9zk96mgOaBpaXRNLNYxnJV0OqMaVKtmIctQu_Iyze_XDyjmzFGlMk-iIPkhcjQn9MKBarGxeACYIZP6tySSytJuKP5AXF10ZNs99nx97RDPn8ieaK3GiycD8IAPLD-n8SucqI7Gp1xAW6PI4u2ANugpyJcgSsOFGbxgNfwToYyon4JMk2uPNfE_xfZryon13aCaA2FVXUxwKk7cjvb8ggmaYleR2LC0ga4GsKSYE8J6dFtuY-ukAQlSlj7vkDhty3y2uqQK6kcbNl3hISrkcS_ejk2uLH5j-ft2XH6qiFbfwtKmVCuiqwxKoZ1-bOetf0IDVCPJPLrafDn4Y2j9p5CM3vpjqjRyYiXN3zjt4gU9fCy9Goet3dJHxe-Lxgj0OoXpQ_YBSSswMd2NXFvobMgeM2JohMx12-_FSMD00KViXjOF6GuQguZLGXh8oaqw2PgXaq59or_q144bLpjkts5oKgJTIY9KaC15S4lQ6phtM9t_V2v6SdO-R-f0cn99Mhdbokr21p4s5cGvsms07e7FiEYlQ3fTXG_jZIRVU7jsrZSwhF7eZN--Ae2rrjMUSfE38oqiBJKp1mXsZFkNMQ-xZNGvvNfBGU=w1000-h625-s-no-gm?authuser=0\");\n        background-size: 90vw 100vh;  # This sets the size to cover 100% of the viewport width and height\n        background-position: center;  \n        background-repeat: no-repeat;\n    }\n    </style>\n    \"\"\"\n\n    st.markdown(background_image, unsafe_allow_html=True)\n\n\n    st.header(\"Login\")\n    username = st.text_input(\"Username\")\n    password = st.text_input(\"Password\", type=\"password\")\n    if st.button(\"Login\"):\n        if authenticate(username, password):\n            st.success(\"Login successful!\") \n            st.session_state['status'].append(\"logged in\")\n\n                               # Redirect to the chatbot page\n        else:\n           st.write('Invalid User Name and Password!')\n           st.write('Create an account if you are new')\n\n              \ndef create_account(full_name, username, password):\n\n\n    # Save the user's credentials\n    # Print the details for now\n    print(\"New Account Created:\")\n    print(\"Full Name:\", full_name)\n    print(\"Username:\", username)\n    print(\"Password:\", password)\n    write_credentials(\"/kaggle/working/project/credentials.json\", username, password)\ndef create():\n   with st.empty().container():\n    background_image = \"\"\"\n    <style>\n    [data-testid=\"stAppViewContainer\"] > .main {\n        background-image: url(\"https://lh3.googleusercontent.com/pw/AP1GczNg67lujMgcHYeTVH3oUhDvFJ9s-kl7RYhxQSIHfbAsQmkh8hPaUCTx3-5O7IVZjy1RICX1NyH1qzg5q4sHxf_1fhtkvG0dAHTb0RKBrxfXfrmVfrnnpdBjVJ-skB4QBW1oCEV4THn0BDhbMzbxrdvIrg5oin8LQ-8JUDKX-2oowUxfzSZnYrGTeXF8EBNQ66k1yuRE36_rScGZrBli_wfkg2Pg0jO0VxTwk4PkmJSAKRpRGTEVXHECgGdLDaHA6JKb34mROZuvrDUZ3hZ-c87P1-_6LJo2bedbb6xhPthUAapdVHcdWv9zk96mgOaBpaXRNLNYxnJV0OqMaVKtmIctQu_Iyze_XDyjmzFGlMk-iIPkhcjQn9MKBarGxeACYIZP6tySSytJuKP5AXF10ZNs99nx97RDPn8ieaK3GiycD8IAPLD-n8SucqI7Gp1xAW6PI4u2ANugpyJcgSsOFGbxgNfwToYyon4JMk2uPNfE_xfZryon13aCaA2FVXUxwKk7cjvb8ggmaYleR2LC0ga4GsKSYE8J6dFtuY-ukAQlSlj7vkDhty3y2uqQK6kcbNl3hISrkcS_ejk2uLH5j-ft2XH6qiFbfwtKmVCuiqwxKoZ1-bOetf0IDVCPJPLrafDn4Y2j9p5CM3vpjqjRyYiXN3zjt4gU9fCy9Goet3dJHxe-Lxgj0OoXpQ_YBSSswMd2NXFvobMgeM2JohMx12-_FSMD00KViXjOF6GuQguZLGXh8oaqw2PgXaq59or_q144bLpjkts5oKgJTIY9KaC15S4lQ6phtM9t_V2v6SdO-R-f0cn99Mhdbokr21p4s5cGvsms07e7FiEYlQ3fTXG_jZIRVU7jsrZSwhF7eZN--Ae2rrjMUSfE38oqiBJKp1mXsZFkNMQ-xZNGvvNfBGU=w1000-h625-s-no-gm?authuser=0\");\n        background-size: 90vw 100vh;  # This sets the size to cover 100% of the viewport width and height\n        background-position: center;  \n        background-repeat: no-repeat;\n    }\n    </style>\n    \"\"\"\n    st.markdown(background_image, unsafe_allow_html=True)\n    st.header(\"Create Account\")\n    full_name = st.text_input(\"Full Name\")\n    new_username = st.text_input(\"New Username\")\n    new_password = st.text_input(\"New Password\", type=\"password\")\n    if st.button(\"Create Account\"):\n      create_account(full_name, new_username, new_password)\n      st.success(\"Account created successfully! Please login.\")\ndef info():\n    background_image = \"\"\"\n    <style>\n    [data-testid=\"stAppViewContainer\"] > .main {\n        background-image: url(\"https://lh3.googleusercontent.com/pw/AP1GczNg67lujMgcHYeTVH3oUhDvFJ9s-kl7RYhxQSIHfbAsQmkh8hPaUCTx3-5O7IVZjy1RICX1NyH1qzg5q4sHxf_1fhtkvG0dAHTb0RKBrxfXfrmVfrnnpdBjVJ-skB4QBW1oCEV4THn0BDhbMzbxrdvIrg5oin8LQ-8JUDKX-2oowUxfzSZnYrGTeXF8EBNQ66k1yuRE36_rScGZrBli_wfkg2Pg0jO0VxTwk4PkmJSAKRpRGTEVXHECgGdLDaHA6JKb34mROZuvrDUZ3hZ-c87P1-_6LJo2bedbb6xhPthUAapdVHcdWv9zk96mgOaBpaXRNLNYxnJV0OqMaVKtmIctQu_Iyze_XDyjmzFGlMk-iIPkhcjQn9MKBarGxeACYIZP6tySSytJuKP5AXF10ZNs99nx97RDPn8ieaK3GiycD8IAPLD-n8SucqI7Gp1xAW6PI4u2ANugpyJcgSsOFGbxgNfwToYyon4JMk2uPNfE_xfZryon13aCaA2FVXUxwKk7cjvb8ggmaYleR2LC0ga4GsKSYE8J6dFtuY-ukAQlSlj7vkDhty3y2uqQK6kcbNl3hISrkcS_ejk2uLH5j-ft2XH6qiFbfwtKmVCuiqwxKoZ1-bOetf0IDVCPJPLrafDn4Y2j9p5CM3vpjqjRyYiXN3zjt4gU9fCy9Goet3dJHxe-Lxgj0OoXpQ_YBSSswMd2NXFvobMgeM2JohMx12-_FSMD00KViXjOF6GuQguZLGXh8oaqw2PgXaq59or_q144bLpjkts5oKgJTIY9KaC15S4lQ6phtM9t_V2v6SdO-R-f0cn99Mhdbokr21p4s5cGvsms07e7FiEYlQ3fTXG_jZIRVU7jsrZSwhF7eZN--Ae2rrjMUSfE38oqiBJKp1mXsZFkNMQ-xZNGvvNfBGU=w1000-h625-s-no-gm?authuser=0\");\n        background-size: 90vw 100vh;  # This sets the size to cover 100% of the viewport width and height\n        background-position: center;  \n        background-repeat: no-repeat;\n    }\n    </style>\n    \"\"\"\n\n    st.markdown(background_image, unsafe_allow_html=True)\n    st.header(\"About\")\n    st.write(\"Legal Assistant is designed to streamline legal processes and provide quick access to legal knowledge. \\\n    Whether you're a lawyer, law student, or just curious about legal matters, \\\n    our platform aims to simplify legal research and assistance.\")\n\n# Contact section\n    st.header(\"Contact Us:\")\n    st.write(\"Have questions or suggestions? Feel free to reach out to us at hashifvs0075@gmail.com\")\n    st.header(\"Contributors of this project:\")\n    st.write(\"Hashif V S, Contact: hashifvs0075@gmail.com\")\n    st.write(\"Aleena James, Contact: aleenakjames@gmail.com\")\n    st.write(\"Priyan V, Contact: priyanvasantha2@gmail.com\")\n    st.write(\"Jomal P Joy, Contact: jomalpjoy@gmail.com\")\n    st.write(\"Neena Joseph, Contact: neenajoseph@sjcetpalai.ac.in\")\n    st.write(\"Source code is available at https://github.com/Hash-if-vs/Legal-Assistant-Chatbot\")\n    \n    st.markdown(\"---\")\n\n   \ndef summarise():\n   background_image = \"\"\"\n    <style>\n    [data-testid=\"stAppViewContainer\"] > .main {\n        background-image: url(\"https://lh3.googleusercontent.com/pw/AP1GczNg67lujMgcHYeTVH3oUhDvFJ9s-kl7RYhxQSIHfbAsQmkh8hPaUCTx3-5O7IVZjy1RICX1NyH1qzg5q4sHxf_1fhtkvG0dAHTb0RKBrxfXfrmVfrnnpdBjVJ-skB4QBW1oCEV4THn0BDhbMzbxrdvIrg5oin8LQ-8JUDKX-2oowUxfzSZnYrGTeXF8EBNQ66k1yuRE36_rScGZrBli_wfkg2Pg0jO0VxTwk4PkmJSAKRpRGTEVXHECgGdLDaHA6JKb34mROZuvrDUZ3hZ-c87P1-_6LJo2bedbb6xhPthUAapdVHcdWv9zk96mgOaBpaXRNLNYxnJV0OqMaVKtmIctQu_Iyze_XDyjmzFGlMk-iIPkhcjQn9MKBarGxeACYIZP6tySSytJuKP5AXF10ZNs99nx97RDPn8ieaK3GiycD8IAPLD-n8SucqI7Gp1xAW6PI4u2ANugpyJcgSsOFGbxgNfwToYyon4JMk2uPNfE_xfZryon13aCaA2FVXUxwKk7cjvb8ggmaYleR2LC0ga4GsKSYE8J6dFtuY-ukAQlSlj7vkDhty3y2uqQK6kcbNl3hISrkcS_ejk2uLH5j-ft2XH6qiFbfwtKmVCuiqwxKoZ1-bOetf0IDVCPJPLrafDn4Y2j9p5CM3vpjqjRyYiXN3zjt4gU9fCy9Goet3dJHxe-Lxgj0OoXpQ_YBSSswMd2NXFvobMgeM2JohMx12-_FSMD00KViXjOF6GuQguZLGXh8oaqw2PgXaq59or_q144bLpjkts5oKgJTIY9KaC15S4lQ6phtM9t_V2v6SdO-R-f0cn99Mhdbokr21p4s5cGvsms07e7FiEYlQ3fTXG_jZIRVU7jsrZSwhF7eZN--Ae2rrjMUSfE38oqiBJKp1mXsZFkNMQ-xZNGvvNfBGU=w1000-h625-s-no-gm?authuser=0\");\n        background-size: 90vw 100vh;  # This sets the size to cover 100% of the viewport width and height\n        background-position: center;  \n        background-repeat: no-repeat;\n    }\n    </style>\n    \"\"\"\n\n   st.markdown(background_image, unsafe_allow_html=True)\n   st.title(\"Summarize\")\n   pdf = st.file_uploader(\"Upload your PDF\", type='pdf')\n   if pdf:\n      pdf_reader=PdfReader(pdf)\n      text=''\n\n      for page in pdf_reader.pages:\n            text += page.extract_text()\n      cleanedtext=text.replace('\\n', '').replace('\\n\\n','')\n      st.write(len(cleanedtext.split(\" \")))\n      text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,\n            chunk_overlap=200,\n            length_function=len\n            )\n      chunks = text_splitter.split_text(text=text)\n      API_URL = \"https://api-inference.huggingface.co/models/allenai/led-large-16384-arxiv\"\n      headers = {\"Authorization\": \"Bearer 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\"}\n\n    \n      prompt_template2 = \"\"\" Instruction: Create a concise summary of the given document capturing the main points and themes.Please read the provided Original section to understand the context and content.\n        Ensure that your final output is thorough, and accurately reflects the document's content and purpose.the content is given below.please generate in less than 150 words\n            user: {content}\n            Answer:\"\"\"\n      prompt = PromptTemplate(input_variables=[\"content\"], template=prompt_template2)\n      llm_chain = LLMChain(llm=sumllm, prompt=prompt)\n      output= llm_chain.run(cleanedtext[:6500])\n   \n\n      \n      st.header(\"Gnereated summary\")\n      st.subheader(output)\n\ndatabase=[]\n\ndef extract_metadata_from_pdf(file_path: str) -> dict:\n    try:    \n  \n        reader = PdfReader(file_path)  # Change this line\n        docmetadata = reader.metadata\n        return {\n                \"title\": str(docmetadata.title)\n                }\n    except PyPDF2.errors.PdfReadError as e:\n        print(f\"Error reading PDF file: {e}\")\n\n\n\ndef extract_pages_from_pdf(file_path: str) -> List[Tuple[int, str]]:\n    \"\"\"\n    Extracts the text from each page of the PDF.\n\n    :param file_path: The path to the PDF file.\n    :return: A list of tuples containing the page number and the extracted text.\n    \"\"\"\n\n    with pdfplumber.open(file_path) as pdf:\n        pages = []\n        for page_num, page in enumerate(pdf.pages):\n            text = page.extract_text()\n            if text.strip():  # Check if extracted text is not empty\n                pages.append((page_num + 1, text))\n    return pages\n\n\ndef parse_pdf(file_path) -> Tuple[List[Tuple[int, str]], Dict[str, str]]:\n    \"\"\"\n    Extracts the title and text from each page of the PDF.\n\n    :param file_path: The path to the PDF file.\n    :return: A tuple containing the title and a list of tuples with page numbers and extracted text.\n    \"\"\"\n \n\n    metadata = extract_metadata_from_pdf(file_path)\n    pages = extract_pages_from_pdf(file_path)\n\n    return pages, metadata\n\n\ndef merge_hyphenated_words(text: str) -> str:\n    return re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", text)\ndef fix_newlines(text: str) -> str:\n    return re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", text)\n\n\ndef remove_multiple_newlines(text: str) -> str:\n    return re.sub(r\"\\n{2,}\", \"\\n\", text)\n\n\ndef clean_text(\n    pages: List[Tuple[int, str]], cleaning_functions: List[Callable[[str], str]]) -> List[Tuple[int, str]]:\n    cleaned_pages = []\n    for page_num, text in pages:\n        for cleaning_function in cleaning_functions:\n            text = cleaning_function(text)\n        cleaned_pages.append((page_num, text))\n    return cleaned_pages\n\n\ndef text_to_docs(text, metadata: Dict[str, str]) ->List[Document]:\n    \"\"\"Converts list of strings to a list of Documents with metadata.\"\"\"\n    doc_chunks=[]\n    for page_num, page in text:\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,\n            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n            chunk_overlap=200,\n        )\n        chunks = text_splitter.split_text(page)\n        for i, chunk in enumerate(chunks):\n            doc = Document(\n                page_content=chunk,\n                metadata={\n                    \"page_number\": page_num,\n                    \"chunk\": i,\n                    \"source\": f\"p{page_num}-{i}\",\n                    **metadata,\n                },\n            )\n            doc_chunks.append(doc)\n\n    return doc_chunks\n\ndef research():\n    st.title(\"Chat with your documents:\")\n\n\n    pdf = st.file_uploader(\"Upload your PDF\", type='pdf')\n    if pdf:\n        raw_pages, metadata = parse_pdf(pdf)\n\n            # Step 2: Create text chunks\n        cleaning_functions = [\n                merge_hyphenated_words,\n                fix_newlines,\n                remove_multiple_newlines,\n                        ]\n        cleaned_text_pdf = clean_text(raw_pages, cleaning_functions)\n        document_chunks = text_to_docs(cleaned_text_pdf, metadata)\n        database.extend(document_chunks)\n\n            # Optional: Reduce embedding cost by only using the first 23 pages\n        model_name = \"BAAI/bge-base-en\"\n        encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n        embeddings = HuggingFaceBgeEmbeddings(\n        model_name=model_name,\n        encode_kwargs=encode_kwargs\n        ) \n            # Step 3 + 4: Generate embeddings and store them in DB\n        vector_store = Chroma.from_documents(\n                database,\n                embeddings,\n                persist_directory=\"db2\",\n            )\n        retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n        sumqa = ConversationalRetrievalChain.from_llm(\n            sumllm,\n            retriever=retriever\n            #memory=memory\n            )\n        with st.empty().container():\n           chat2(sumqa)\n\n\ndef main():\n\n  with st.sidebar:\n    selected = option_menu(\n      menu_title=\"Main Menu\",\n      options = [\"Home\",\"login\",\"Create Account\",\"Chat\",\"Summarise\",\"Research\",\"About\"],\n      icons = [\"house\",\"key\",\"person-add\",\"chat-dots\",\"file-earmark-arrow-up\",\"file\",\"info\"]\n    )\n  if selected == \"Home\":\n    with st.empty().container():\n       home()\n  if selected == \"login\":\n    with st.empty().container():\n       login()\n  if selected == \"Create Account\":\n    with st.empty().container():\n       create()\n  if selected == \"Chat\":\n    with st.empty().container():\n        chat(qa)\n        pass\n\n  if selected == \"Summarise\":\n    with st.empty().container():\n        summarise()\n\n  if selected == \"Research\":\n    with st.empty().container():\n        research()     \n  if selected == \"About\":\n     info()\n\n       \n      \nif __name__ == \"__main__\":\n    main()\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:18:19.311720Z","iopub.execute_input":"2024-06-13T10:18:19.312116Z","iopub.status.idle":"2024-06-13T10:18:19.332772Z","shell.execute_reply.started":"2024-06-13T10:18:19.312086Z","shell.execute_reply":"2024-06-13T10:18:19.331862Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Writing app.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!wget -q -O - ipv4.icanhazip.com\n!streamlit run /kaggle/working/app.py & npx localtunnel --port 8501\n\"\"\"After running this line copy the ip address, here it is 35.223.163.127\n   Go to the url you get at the end\n    paste your ip address there to access your tunnel website application\n    \"\" ","metadata":{"execution":{"iopub.status.busy":"2024-05-08T05:25:16.965495Z","iopub.execute_input":"2024-05-08T05:25:16.966246Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"35.223.163.127\n\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n\u001b[0m\n\u001b[0m\n\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n\u001b[0m\n\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.19.2.2:8502\u001b[0m\n\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.223.163.127:8502\u001b[0m\n\u001b[0m\nyour url is: https://brave-shoes-melt.loca.lt\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}