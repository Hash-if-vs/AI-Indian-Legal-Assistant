{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PREPROCESSING\n<br>\nPreprocessing data is very crucial for the performance of RAG, given below are the list of things we do as a part of preprocesing our data :\n<br>\n- Extract text and metadata\n<br>\n- Data Cleaning\n<br>\n- Document Chunking\n<br>\n- Create Embedding Vectors & Vector Database\n<br>\nThe data contains Indian Government acts in pdf format. \n","metadata":{}},{"cell_type":"code","source":"#install dependencies\n!pip install -q pypdf2 pdfplumber langchain\n!pip install -q -U langchain-community\n!pip install -q sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-06-11T18:53:51.118218Z","iopub.execute_input":"2024-06-11T18:53:51.118656Z","iopub.status.idle":"2024-06-11T18:54:27.782071Z","shell.execute_reply.started":"2024-06-11T18:53:51.118619Z","shell.execute_reply":"2024-06-11T18:54:27.780730Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#import modules\nimport os\nimport PyPDF2.errors\nimport pdfplumber\nfrom typing import Callable, List, Tuple, Dict\nfrom PyPDF2 import PdfReader\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nimport re\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nimport PyPDF2","metadata":{"execution":{"iopub.status.busy":"2024-06-11T16:46:33.695732Z","iopub.execute_input":"2024-06-11T16:46:33.696649Z","iopub.status.idle":"2024-06-11T16:46:33.703132Z","shell.execute_reply.started":"2024-06-11T16:46:33.696605Z","shell.execute_reply":"2024-06-11T16:46:33.701848Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Extract text and metadata from the documents.\n- Read metadata (specifically the title) from a PDF file.\n- Extract text from each page of a PDF file.\n- Combine the functionalities of extracting metadata and extracting text from a PDF file.\n","metadata":{}},{"cell_type":"code","source":"#Reads metadata, specifically the title, from a PDF file using PyPDF2.\n\ndef extract_metadata_from_pdf(file_path: str) -> dict:\n    try:\n        with open(file_path, \"rb\") as pdf_file:\n            # Open PDF file using PyPDF2\n            reader = PdfReader(pdf_file) \n            docmetadata = reader.metadata\n            # Extract and return the title from the metadata as a dictionary\n            return {\"title\": str(docmetadata.title)}\n    except PyPDF2.errors.PdfReadError as e:\n        # Handle PdfReadError exceptions\n        print(f\"Error reading PDF file: {e}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-11T17:09:30.866518Z","iopub.execute_input":"2024-06-11T17:09:30.867279Z","iopub.status.idle":"2024-06-11T17:09:30.873580Z","shell.execute_reply.started":"2024-06-11T17:09:30.867249Z","shell.execute_reply":"2024-06-11T17:09:30.872347Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def extract_pages_from_pdf(file_path: str) -> List[Tuple[int, str]]:\n    \"\"\"\n    Extracts the text from each page of the PDF.\n\n    :param file_path: The path to the PDF file.\n    :return: A list of tuples containing the page number and the extracted text.\n    \"\"\"\n    if not os.path.isfile(file_path):\n        # Raise an error if the file does not exist\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n\n    with pdfplumber.open(file_path) as pdf:\n        pages = []\n        for page_num, page in enumerate(pdf.pages):\n            # Extract text from each page\n            text = page.extract_text()\n            # Check if extracted text is not empty\n            if text.strip():  \n                # Append page number and extracted text to the list\n                pages.append((page_num + 1, text))\n    return pages","metadata":{"execution":{"iopub.status.busy":"2024-06-11T17:09:33.419269Z","iopub.execute_input":"2024-06-11T17:09:33.419927Z","iopub.status.idle":"2024-06-11T17:09:33.426361Z","shell.execute_reply.started":"2024-06-11T17:09:33.419888Z","shell.execute_reply":"2024-06-11T17:09:33.425302Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def parse_pdf(file_path: str) -> Tuple[List[Tuple[int, str]], Dict[str, str]]:\n    \"\"\"\n    Extracts the title and text from each page of the PDF.\n\n    :param file_path: The path to the PDF file.\n    :return: A tuple containing the title and a list of tuples with page numbers and extracted text.\n    \"\"\"\n    if not os.path.isfile(file_path):\n        # Raise an error if the file does not exist\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n\n    # Extract metadata from the PDF file\n    metadata = extract_metadata_from_pdf(file_path)\n    # Extract text from each page of the PDF file\n    pages = extract_pages_from_pdf(file_path)\n\n    return pages, metadata","metadata":{"execution":{"iopub.status.busy":"2024-06-11T17:09:36.008397Z","iopub.execute_input":"2024-06-11T17:09:36.008805Z","iopub.status.idle":"2024-06-11T17:09:36.015344Z","shell.execute_reply.started":"2024-06-11T17:09:36.008776Z","shell.execute_reply":"2024-06-11T17:09:36.014122Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning\n- Merge Hyphenated Words: Combines hyphenated words separated by a newline character into a single word.\n- Fix Newlines: Corrects newlines in the text by replacing single newlines with spaces.\n- Remove Multiple Newlines: Eliminates consecutive multiple newlines from the text.","metadata":{}},{"cell_type":"code","source":"\ndef merge_hyphenated_words(text: str) -> str:\n    \"\"\"\n    Merges hyphenated words separated by a newline character in a text.\n    \n    :param text: The input text.\n    :return: The text with hyphenated words merged.\n    \"\"\"\n    return re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", text)\n\n\ndef fix_newlines(text: str) -> str:\n    \"\"\"\n    Fixes newlines in the text by replacing single newlines with spaces.\n    \n    :param text: The input text.\n    :return: The text with fixed newlines.\n    \"\"\"\n    return re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", text)\n\n\ndef remove_multiple_newlines(text: str) -> str:\n    \"\"\"\n    Removes consecutive multiple newlines from the text.\n    \n    :param text: The input text.\n    :return: The text with consecutive multiple newlines removed.\n    \"\"\"\n    return re.sub(r\"\\n{2,}\", \"\\n\", text)\ndef clean_text(\n    pages: List[Tuple[int, str]], cleaning_functions: List[Callable[[str], str]]\n) -> List[Tuple[int, str]]:\n    \"\"\"\n    Cleans the text of each page using a list of cleaning functions.\n\n    Args:\n        pages (List[Tuple[int, str]]): List of tuples where each tuple contains a page number and the corresponding text.\n        cleaning_functions (List[Callable[[str], str]]): List of functions to clean the text. Each function takes a string and returns a cleaned string.\n\n    Returns:\n        List[Tuple[int, str]]: List of tuples with the cleaned text.\n    \"\"\"\n    cleaned_pages = []  # Initialize an empty list to hold the cleaned pages.\n\n    for page_num, text in pages:  # Iterate over each page in the pages list.\n        for cleaning_function in cleaning_functions:  # Apply each cleaning function to the text.\n            text = cleaning_function(text)  # Update the text with the cleaned version.\n        \n        cleaned_pages.append((page_num, text))  # Add the cleaned page to the list.\n\n    return cleaned_pages  # Return the list of cleaned pages.\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T17:57:47.881442Z","iopub.execute_input":"2024-06-11T17:57:47.882141Z","iopub.status.idle":"2024-06-11T17:57:47.890626Z","shell.execute_reply.started":"2024-06-11T17:57:47.882109Z","shell.execute_reply":"2024-06-11T17:57:47.889512Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Document Chunking\n- The document is split into chunks, with a specified chunk size.\n- Chunk size significantly impacts search results.\n- Large chunks may result in vectors that are too generalized, losing specificity.\n- Small chunks may lose the context necessary for accurate understanding.\n- Optimal chunk size balances specificity and context, enhancing search effectiveness.\n","metadata":{}},{"cell_type":"code","source":"def text_to_docs(text, metadata: Dict[str, str]) -> List[Document]:\n    \"\"\"\n    Converts a list of strings to a list of Documents with metadata.\n\n    Args:\n        text (List[Tuple[int, str]]): List of tuples where each tuple contains a page number and the corresponding text.\n        metadata (Dict[str, str]): Dictionary containing additional metadata to be added to each Document.\n\n    Returns:\n        List[Document]: List of Document objects with chunked text and metadata.\n    \"\"\"\n    doc_chunks = []  # Initialize an empty list to hold the Document chunks.\n    \n    for page_num, page in text:  # Iterate over each page in the text list.\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,  # Maximum size of each chunk.\n            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],  # Characters to split the text.\n            chunk_overlap=200,  # Number of overlapping characters between chunks.\n        )\n        \n        chunks = text_splitter.split_text(page)  # Split the text into chunks.\n        \n        for i, chunk in enumerate(chunks):  # Iterate over each chunk.\n            doc = Document(\n                page_content=chunk,  # Set the content of the Document to the current chunk.\n                metadata={\n                    \"page_number\": page_num,  # Add the page number to the metadata.\n                    \"chunk\": i,  # Add the chunk index to the metadata.\n                    \"source\": f\"p{page_num}-{i}\",  # Create a source identifier for the chunk.\n                    **metadata,  # Include additional metadata passed to the function.\n                },\n            )\n            doc_chunks.append(doc)  # Add the Document to the list of chunks.\n\n    return doc_chunks  # Return the list of Document chunks.\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T17:37:27.268565Z","iopub.execute_input":"2024-06-11T17:37:27.268935Z","iopub.status.idle":"2024-06-11T17:37:27.276904Z","shell.execute_reply.started":"2024-06-11T17:37:27.268907Z","shell.execute_reply":"2024-06-11T17:37:27.275874Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Create Embedding Vectors & Vector Database\n\n1. **Convert Texts to Embedding Vectors:**\n   - BAAI/bge-base-en is used to transform each chunk of text into an embedding vector.\n   - Embedding vectors are numerical representations that encapsulate the semantic meaning of the text.\n\n2. **Store Embeddings into a Vector Database:**\n   - Chromadb is used to save the embedding vectors, enabling efficient and scalable retrieval.\n   - Here the database is being stored locally u can tweak the code to store the db in the cloud storage provided by chroma, or use other vector databases.\n\n","metadata":{}},{"cell_type":"code","source":"#load the embedding model\nmodel_name = \"BAAI/bge-base-en\" # try out alternative models available on huggingface\nencode_kwargs = {\"normalize_embeddings\": True}  # Set to True to compute cosine similarity\nembeddings = HuggingFaceBgeEmbeddings(\n    model_name=model_name, encode_kwargs=encode_kwargs\n)\n\n# Specify the path to the data directory\nroot_directory = \" \"\n\ndatabase = []  # Initialize a list to store document chunks\n\n# Iterate through the files in the root directory\nfor root, directories, files in os.walk(root_directory):\n    for filename in files:\n        # Construct the full path to the current file\n        file_path = os.path.join(root, filename[0:])\n        file_path = file_path.replace(\"\\\\\", \"/\")\n        print(file_path)  # Print the file path for reference\n\n        # Step 1: Parse PDF\n        raw_pages, metadata = parse_pdf(file_path)  # Parse the PDF file\n\n        # Step 2: Create text chunks\n        cleaning_functions = [\n            merge_hyphenated_words,\n            fix_newlines,\n            remove_multiple_newlines,\n        ]  # Define cleaning functions\n        cleaned_text_pdf = clean_text(raw_pages, cleaning_functions)  # Clean the text\n        document_chunks = text_to_docs(cleaned_text_pdf, metadata)  # Split text into document chunks\n        database.extend(document_chunks)  # Add document chunks to the database\n\n        if len(database) > 2000:  # Check if database size exceeds a threshold \n\n            # Store embeddings in Chroma vector database\n            vector_store = Chroma.from_documents(\n                database,\n                embeddings,\n                persist_directory=\"\",  # Specify the directory for storing the vector database\n            )\n            vector_store.persist()  # Persist the vector database\n\n            database = []  # Reset the database for the next batch of documents\n","metadata":{},"execution_count":null,"outputs":[]}]}