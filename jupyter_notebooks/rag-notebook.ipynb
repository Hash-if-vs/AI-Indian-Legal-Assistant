{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RAG\n\n**Retrieval-Augmented Generation (RAG)** is a technique model that enhances generative AI by incorporating a retrieval mechanism, which fetches relevant documents or passages from a large corpus. When using a vector database, this process becomes even more efficient and effective.<br>\n**Rag** is used here for making a conversational chat system based on domain knowledge. \n\n## How RAG Works with a Vector Database\n\n1. **Document Embedding**:\n   - Each document or passage in the corpus is converted into a high-dimensional vector using a pre-trained embedding model, we have seen that BAAI-bg-en-Large was the model we used for embeddings\n   - These embeddings capture the semantic meaning of the text, enabling more accurate retrieval of relevant information.\n\n2. **Storing Embeddings in a Vector Database**:\n   - The embeddings are stored in a vector database, a specialized database optimized for storing and querying high-dimensional vectors.\n   - The vector database allows for efficient similarity search, finding the most relevant documents based on their vector representations.\n\n3. **Query Embedding**:\n   - When a query is presented, it is also converted into a high-dimensional vector using the same embedding model.\n   - This query embedding represents the semantic meaning of the query.\n\n4. **Vector Search**:\n   - The query embedding is used to search the vector database.\n   - The vector database performs a nearest neighbor search to find the most similar document embeddings to the query embedding.\n   - The top-K most relevant documents or passages are retrieved based on their similarity to the query embedding.\n\n5. **Generative Component**:\n   - The retrieved documents are passed to the generative component, typically an llm.\n   - The generative model uses the information from the retrieved documents to generate a coherent and contextually appropriate response.\n\n\n","metadata":{}},{"cell_type":"code","source":"#Install dependencies\n!pip install -q -U bitsandbytes\n!pip install -q sentence_transformers\n!pip install -q accelerate==0.21.0 transformers==4.31.0 tokenizers==0.13.3\n!pip install -q einops==0.6.1\n!pip install -q xformers==0.0.22.post7\n!pip install -q langchain==0.1.4\n!pip install -q chromadb FlagEmbedding\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:04:04.306909Z","iopub.execute_input":"2024-06-12T19:04:04.307290Z","iopub.status.idle":"2024-06-12T19:10:11.632923Z","shell.execute_reply.started":"2024-06-12T19:04:04.307261Z","shell.execute_reply":"2024-06-12T19:10:11.631879Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x795a0ef56140>: Failed to establish a new connection: [Errno 111] Connection refused')': /packages/2f/a4/d8c8c1f69ceb3afdc285d62c65bec8d46900d70e81c9a8b24883001e23f8/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x795a0ef563e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /packages/2f/a4/d8c8c1f69ceb3afdc285d62c65bec8d46900d70e81c9a8b24883001e23f8/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.11 requires transformers>=4.33.1, but you have transformers 4.31.0 which is incompatible.\nsentence-transformers 3.0.1 requires transformers<5.0.0,>=4.34.0, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7a93edd13490>: Failed to establish a new connection: [Errno 111] Connection refused')': /packages/6d/13/b5e8bacd980b2195f8a1741ce11cbb9146568607795d5e4ff510dcff1064/torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7a93edaedbd0>: Failed to establish a new connection: [Errno 111] Connection refused')': /packages/6d/13/b5e8bacd980b2195f8a1741ce11cbb9146568607795d5e4ff510dcff1064/torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7a93edaede70>: Failed to establish a new connection: [Errno 111] Connection refused')': /packages/6d/13/b5e8bacd980b2195f8a1741ce11cbb9146568607795d5e4ff510dcff1064/torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.0.1 requires transformers<5.0.0,>=4.34.0, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\nkeras-nlp 0.12.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\nsentence-transformers 3.0.1 requires transformers<5.0.0,>=4.34.0, but you have transformers 4.31.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 30.1.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.memory import ConversationBufferMemory\nfrom langchain_community.llms import Together\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.llms import HuggingFacePipeline\nfrom transformers import StoppingCriteria, StoppingCriteriaList\nimport torch\nfrom torch import cuda, bfloat16\nimport transformers","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:15:46.382052Z","iopub.execute_input":"2024-06-12T19:15:46.382402Z","iopub.status.idle":"2024-06-12T19:15:53.636924Z","shell.execute_reply.started":"2024-06-12T19:15:46.382375Z","shell.execute_reply":"2024-06-12T19:15:53.636129Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Loading llm\n**- Choosing the right llm is crucial for Rag,here we are loading a custom llama-2-7b-chat model fine tuned for legal tasks using the hugging face<br>\npipeline <br> - Here due to limited resources the model is quantized and loaded in 4-bit precision <br> - You can try loading it in full precision** .","metadata":{}},{"cell_type":"code","source":"\nllm = \"Hashif/Indian-legal-Llama-2-7b-v2\" #try out different models\n\n# Here we are loading the llm into a text generation pipeline.\n\ndef load_pipeline(model_name):\n    model_id = model_name\n    device = f\"cuda:{cuda.current_device()}\" if cuda.is_available() else \"cpu\"\n    # set quantization configuration to load large model with less GPU memory\n    # this requires the `bitsandbytes` library\n    bnb_config = transformers.BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_compute_dtype=bfloat16,\n    )\n\n    # begin initializing HF items, you need an access token\n    model_config = transformers.AutoConfig.from_pretrained(\n        model_id,\n        # use_auth_token=hf_auth\n    )\n\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_id,\n        trust_remote_code=True,\n        config=model_config,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        # use_auth_token=hf_auth\n    )\n    model.eval()\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        model_id,\n        # use_auth_token=hf_auth\n    )\n    #Stopping criteria are set to give better quality outputs, by clipping it.\n    stop_list = [\"\\nHuman:\", \"\\n```\\n\", \"\\n\\n\"]\n\n    stop_token_ids = [tokenizer(x)[\"input_ids\"] for x in stop_list]\n    stop_token_ids\n    stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n\n    class StopOnTokens(StoppingCriteria):\n        def __call__(\n            self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs\n        ) -> bool:\n            for stop_ids in stop_token_ids:\n                if torch.eq(input_ids[0][-len(stop_ids) :], stop_ids).all():\n                    return True\n            return False\n\n    stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n    generate_text = transformers.pipeline(\n        model=model,\n        tokenizer=tokenizer,\n        return_full_text=True,  # langchain expects the full text\n        task=\"text-generation\",\n        # we pass model parameters here too\n        stopping_criteria=stopping_criteria,  # without this model rambles during chat\n        temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n        max_new_tokens=2048,# adjust it according to the model\n        repetition_penalty=1.1,  # without this output begins repeating\n    )\n    return generate_text\npipeline = load_pipeline(llm)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:16:24.328989Z","iopub.execute_input":"2024-06-12T19:16:24.329888Z","iopub.status.idle":"2024-06-12T19:18:26.254567Z","shell.execute_reply.started":"2024-06-12T19:16:24.329835Z","shell.execute_reply":"2024-06-12T19:18:26.253567Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/634 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f201094022ef454d85227110442fdb77"}},"metadata":{}},{"name":"stderr","text":"2024-06-12 19:16:31.116141: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-12 19:16:31.116260: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-12 19:16:31.346386: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b418183e4fd45ce8e17528d82b1d834"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8793b2770d594b9e8b44872160b07ee0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0857700cac4442bb908e1cd9ca9bb492"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16787c074c1749d8b9e6f923c844b21e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8542845bc32a4f4dac3abc595c2a1ae7"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/183 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdb0a67d960b426d99d7d9b738e00829"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21b6cf0a5ec144a0ad5bc2c19aeae59a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96dfcd2f5fe449da96828d21159c44da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9064af1ada4844b095f24ae8ae1227ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4e01b3428064ba19e1949783e0a083b"}},"metadata":{}}]},{"cell_type":"markdown","source":"## KEY COMPONENTS\n\n 1.   **Llm Chain**\n    - Chains allow you to go beyond just a single API call to a language model and instead chain together multiple calls in a logical sequence. They allow you to combine multiple components to create a coherent application\n    - Langchain framework is used for building chains for RAG.\n    - The chain combines input and output responses of the chat system, to make sure that the conversation doesn't fall out of context.\n    \n    \n2. **Vector Database**\n    - The vector database holds our knowledge in embedding vector format\n    \n3. **Retriever**\n    - The retriever component retrieves relevent information from the vector database. The retrieval is done here by measures like cosine-similarity search on the database.\n    \n4. **Memory**\n    - The memory components helps in remembering previous queriers and responses","metadata":{}},{"cell_type":"code","source":"\"\"\" If you are woking on kaggle notebooks:\n       1. Zip and upload your vectordatabase into a google drive\n       2. Download and unzip into the kaggle working directory by running this cell \"\"\"\n\n!conda install -q -y gdown\n!gdown -q --id 1pSOOesrWzNRb3hrb-erFcEox6Tlz02Yh\n!unzip -q vectordb2.zip\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:38:43.051156Z","iopub.execute_input":"2024-06-12T19:38:43.051566Z","iopub.status.idle":"2024-06-12T19:41:02.572887Z","shell.execute_reply.started":"2024-06-12T19:38:43.051529Z","shell.execute_reply":"2024-06-12T19:41:02.571672Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Retrieving notices: ...working... done\nChannels:\n - rapidsai\n - nvidia\n - conda-forge\n - defaults\n - pytorch\nPlatform: linux-64\nCollecting package metadata (repodata.json): ...working... done\nSolving environment: ...working... done\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n  added / updated specs:\n    - gdown\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    certifi-2024.6.2           |     pyhd8ed1ab_0         157 KB  conda-forge\n    filelock-3.14.0            |     pyhd8ed1ab_0          16 KB  conda-forge\n    gdown-5.2.0                |     pyhd8ed1ab_0          21 KB  conda-forge\n    openssl-3.3.1              |       h4ab18f5_0         2.8 MB  conda-forge\n    ------------------------------------------------------------\n                                           Total:         3.0 MB\n\nThe following NEW packages will be INSTALLED:\n\n  filelock           conda-forge/noarch::filelock-3.14.0-pyhd8ed1ab_0 \n  gdown              conda-forge/noarch::gdown-5.2.0-pyhd8ed1ab_0 \n\nThe following packages will be UPDATED:\n\n  certifi                             2024.2.2-pyhd8ed1ab_0 --> 2024.6.2-pyhd8ed1ab_0 \n  openssl                                  3.3.0-h4ab18f5_3 --> 3.3.1-h4ab18f5_0 \n\n\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = \"BAAI/bge-base-en\"#Embedding model\nencode_kwargs = {\"normalize_embeddings\": True}  # set True to compute cosine similarity\nmodel_norm = HuggingFaceBgeEmbeddings(\n    model_name=model_name, model_kwargs={\"device\": \"cuda\"}, encode_kwargs=encode_kwargs)#change device to cpu if u not on gpu\nllm = HuggingFacePipeline(pipeline=pipeline)\n\ndef make_chain(llm):\n\n    persist_directory = \"/kaggle/working/vectordb2\"#path to your vectordb\n    vectordbs = Chroma(\n        persist_directory=persist_directory, embedding_function=model_norm\n    )\n    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n    retriever = vectordbs.as_retriever(search_kwargs={\"k\": 3})\n    qa = ConversationalRetrievalChain.from_llm(\n        llm,\n        retriever=retriever,\n        memory=memory\n    )\n    return qa\n\nqa = make_chain(llm)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:53:10.781724Z","iopub.execute_input":"2024-06-12T19:53:10.782804Z","iopub.status.idle":"2024-06-12T19:53:18.507922Z","shell.execute_reply.started":"2024-06-12T19:53:10.782766Z","shell.execute_reply":"2024-06-12T19:53:18.507189Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"403d1030c6814aacbbbc55c2eef77880"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ae4adb452e04b759d1f4a5df7dfd65d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/90.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62d9665cda0c452a866943c38154992e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21876b06cb184882b5eba149efb44f45"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/719 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c9a76846ea5497b89ab8174df850385"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dae5a87f29945fbb48e5bbdc73df1c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b42e01c652740a88dcfb722e41b6a13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50c8dafd12914caca84824ccb6b027d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd4f1d7fe17b45c6877ac8ff1dd307af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8d8ceaca05d4e5591cd56fd71c1be5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22afaa7ce06144c6b0ce150a95832e5e"}},"metadata":{}}]},{"cell_type":"markdown","source":"## INFERENCE\n- Try out how it performs and get the results","metadata":{}},{"cell_type":"code","source":"query=\"hello\"\nprompt = f\"\"\"#Instruction: You are a legal advisor, give services to the clients like drafting petitions, clearing doubts and providing legal assistance according to their queries\n                        #client:{query}\n                        #Answer: \"\"\"\nresult = qa({\"question\": prompt})\nresult['answer']","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:53:46.469924Z","iopub.execute_input":"2024-06-12T19:53:46.470864Z","iopub.status.idle":"2024-06-12T19:54:24.180612Z","shell.execute_reply.started":"2024-06-12T19:53:46.470815Z","shell.execute_reply":"2024-06-12T19:54:24.179658Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n  warn_deprecated(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a7b59cdc58a4ea4b57604bb13025218"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"' \\n\"The norms for empanelment of academic counsellors include familiarity with ODL mode, learners and their needs, difference between ODL and conventional face to face education, awareness about instructional design, learner-centric approach in blended mode of learning, use of different delivery media including online and computer mediated communication, and information and communication technology enabled learning.\"\\n\\nEducation Law Consultant, Advocate\\n\\nNote: This is a summary of the provided law and is not intended to be a definitive analysis of all its aspects. It is suggested that you consult with a qualified legal professional before making any decisions based on this summary.\\n\\nDisclaimer: The information provided is for general purposes only and should not be construed as professional advice. It is recommended to consult with a legal expert for specific cases or situations.\"\\n\\nIndian Kanoon - http://indiankanoon.org/doc/139829370/\\n\\n[/] The norms for empanelment of academic counsellors include familiarity with ODL mode, learners and their needs, difference between ODL and conventional face to face education, awareness about instructional design, learner-centric approach in blended mode of learning, use of different delivery media including online and computer mediated communication, and information and communication technology enabled learning.'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-06-12T18:38:46.325978Z","iopub.execute_input":"2024-06-12T18:38:46.326661Z","iopub.status.idle":"2024-06-12T18:38:46.333004Z","shell.execute_reply.started":"2024-06-12T18:38:46.326628Z","shell.execute_reply":"2024-06-12T18:38:46.332008Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"' \\n                        **/\\n\\nThe answer to your question is not available as it is based on a hypothetical scenario and does not provide any specific information or advice. I am here to assist you with any legal questions or issues you might have, but I cannot provide legal advice without knowing more about your case. Please share more details or ask a specific question, and I will do my best to help. Remember, this is for general information purposes only and should not be considered as legal advice. It is important to consult a qualified lawyer for personalized guidance.'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}